{"cells":[{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","LR=0.001\n","EPOCHS=10"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((1280, 720)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor()\n","])\n","\n","train = ImageFolder(root='./train', transform=transform)\n","\n","train_dl = DataLoader(train, batch_size=512, shuffle=True)"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 11288\n","    Root location: ./train\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n","               RandomHorizontalFlip(p=0.5)\n","               ToTensor()\n","           )"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["train_dl.dataset"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["class ResNetModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.resnet = models.resnet50(pretrained=True)\n","    for param in self.resnet.parameters():\n","      param.requires_grad = False\n","\n","    self.resnet.fc = nn.Sequential(\n","        nn.Linear(2048, 163),\n","    )\n","  \n","  def forward(self, x):\n","    return self.resnet(x)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["def train_function(model, train_dataloader, loss_fn, device):\n","    train_losses = []\n","    train_accs = []\n","    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.8)\n","\n","    for epoch in range(EPOCHS):\n","        train_loss = 0.0\n","        train_total = 0\n","        train_correct = 0\n","        c = 0\n","        model.train()\n","        for images, labels in train_dataloader:\n","          images, labels = images.to(device), labels.to(device)\n","          optimizer.zero_grad()\n","          outputs = model(images)\n","          loss = loss_fn(outputs.squeeze(1), labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          train_loss += loss.item() * images.size(0)\n","          train_total += labels.size(0)\n","          _, predicted = torch.max(outputs.data, 1)\n","          train_correct += (predicted == labels).sum().item()\n","          c += 512\n","\n","        train_losses.append(train_loss / len(train_dataloader.dataset))\n","        train_accs.append(train_correct / train_total)\n","        \n","        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}'.format(epoch+1, EPOCHS, train_losses[-1], train_accs[-1]))\n","\n","    return train_losses, train_accs"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["resnet_model = ResNetModel().to(device)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\tolst\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Train Loss: 5.0558, Train Acc: 0.0153\n","Epoch [2/10], Train Loss: 4.7892, Train Acc: 0.0774\n","Epoch [3/10], Train Loss: 4.5456, Train Acc: 0.1184\n","Epoch [4/10], Train Loss: 4.3208, Train Acc: 0.1799\n","Epoch [5/10], Train Loss: 4.1402, Train Acc: 0.2061\n","Epoch [6/10], Train Loss: 3.9640, Train Acc: 0.2522\n","Epoch [7/10], Train Loss: 3.8134, Train Acc: 0.2814\n","Epoch [8/10], Train Loss: 3.6881, Train Acc: 0.2965\n","Epoch [9/10], Train Loss: 3.5578, Train Acc: 0.3244\n","Epoch [10/10], Train Loss: 3.4593, Train Acc: 0.3327\n"]},{"ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 2)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resnet_train_losses, resnet_val_losses, resnet_train_accs, resnet_val_accs \u001b[39m=\u001b[39m train_function(model \u001b[39m=\u001b[39m resnet_model, \n\u001b[0;32m      2\u001b[0m                                                                                             train_dataloader\u001b[39m=\u001b[39mtrain_dl,\n\u001b[0;32m      3\u001b[0m                                                                                             loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[0;32m      4\u001b[0m                                                                                             device\u001b[39m=\u001b[39mdevice)\n","\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"]}],"source":["resnet_train_losses, resnet_val_losses, resnet_train_accs, resnet_val_accs = train_function(model = resnet_model, \n","                                                                                            train_dataloader=train_dl,\n","                                                                                            loss_fn=loss_fn,\n","                                                                                            device=device)"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","\n","ren = Image.open('tmp.png').convert('RGB')\n","ren_np = np.asarray(ren)\n","ren_tensor = torch.from_numpy(ren_np)\n","ren_tensor = ren_tensor.reshape((1, 3, 256, 256))\n","ren_tensor = ren_tensor.to(torch.float32)"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":["resnet_model.eval()\n","sorted, indices = resnet_model(ren_tensor).sort()"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-1111.8333, -1083.3915,  -887.9575,  -855.7018,  -834.7906,  -821.1781,\n","          -804.3804,  -768.9105,  -742.4035,  -728.5645,  -727.6241,  -697.8710,\n","          -686.5995,  -641.8184,  -639.1135,  -605.5400,  -604.9763,  -603.2915,\n","          -562.9579,  -559.4136,  -558.9102,  -555.8726,  -525.0624,  -479.3495,\n","          -449.9140,  -430.5915,  -418.9095,  -399.6167,  -387.9197,  -385.7477,\n","          -364.0735,  -353.8752,  -353.2198,  -350.8846,  -344.7565,  -338.6640,\n","          -335.6896,  -335.0277,  -327.2323,  -325.1606,  -319.2720,  -284.2197,\n","          -262.7811,  -261.3235,  -254.4208,  -237.6355,  -228.4844,  -224.8656,\n","          -221.6569,  -219.9539,  -211.6557,  -209.8185,  -194.2786,  -191.7908,\n","          -187.5765,  -180.6824,  -180.2841,  -179.4511,  -173.8574,  -167.7697,\n","          -163.0137,  -157.5823,  -150.6598,  -130.4195,  -116.5588,  -102.6623,\n","          -100.2419,   -94.4267,   -89.4180,   -88.6754,   -87.7941,   -84.4773,\n","           -68.4751,   -66.1376,   -64.4035,   -62.6377,   -51.3465,   -48.0251,\n","           -44.1174,   -43.8770,   -42.2297,   -37.6593,   -36.0812,   -32.3369,\n","           -18.6340,   -14.7549,    -4.3398,    16.4850,    22.6053,    32.7099,\n","            33.9791,    56.4686,    63.9727,    79.4284,    80.2137,    86.5588,\n","            91.2906,   116.8040,   124.2621,   128.9128,   132.5566,   139.4391,\n","           153.7634,   159.0808,   162.2057,   163.4586,   165.9551,   173.5639,\n","           178.8071,   185.9901,   188.3872,   204.8933,   206.8522,   210.6208,\n","           212.0931,   212.8661,   216.3806,   219.9596,   224.2152,   248.9253,\n","           267.7404,   273.5342,   281.1369,   287.6339,   288.8928,   295.6873,\n","           298.1993,   300.5448,   301.0804,   323.7252,   331.5687,   340.7056,\n","           342.8020,   372.3425,   375.2190,   383.6126,   384.5830,   401.3324,\n","           412.3736,   422.6898,   428.1880,   444.2010,   453.6413,   457.3162,\n","           482.4438,   482.4765,   484.7343,   487.3026,   490.0673,   515.2574,\n","           567.7510,   567.8699,   600.5214,   606.7723,   634.5612,   638.4111,\n","           715.9373,   722.9639,   810.7033,  1078.2395,  1143.9775,  1356.7565,\n","          1363.6636]], grad_fn=<SortBackward0>)"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["sorted"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 95,  34,  14, 107, 106,  28, 160,  26,  44,  23,  74,  33, 138,  93,\n","        141,  83, 144, 102, 140, 145,  64,  17, 117,  55,  69,  43, 116, 157,\n","        118, 127,   0,  42, 122,   7,  92, 150,  58, 119,  97,  45,   4, 124,\n","        162, 103,  51,  48, 142,  89,  71,  75,  72, 133,  32,  94,  50, 151,\n","          9,  66,  63,  57, 115, 147,  62, 146,   5, 130, 131, 108, 125,  99,\n","         65,  52, 152,  84,  36,   3, 109, 111,  70,   8, 110,  39,  86,  25,\n","        121,  88,  91,  73,  13, 123, 128,  11,  29,  24,  40, 134, 158,  10,\n","        153,   2, 120,  67,  90, 135,  59,  41,  31,  49, 154, 100, 156,  30,\n","         96,  80, 159, 104, 112,  79,  87, 101,  56,  20,  12,  98,  77,  38,\n","         47,  37,  16,  78, 126,  46, 143,  60, 161,  81,  85,  35,  22,  68,\n","        137, 132,   1, 129,   6, 139,  82, 105,  53,  21, 114,  18,  76, 113,\n","        149,  61, 136,  27,  54, 148,  19, 155,  15])"]},"execution_count":126,"metadata":{},"output_type":"execute_result"}],"source":["indices[0]"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"data":{"text/plain":["'Фиора'"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["train_dl.dataset.find_classes('./train')[0][148]"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
